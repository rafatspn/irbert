# -*- coding: utf-8 -*-
"""IRBERT_RP_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVGxCCg8b-mUcH5wyn9Uv4VK-EB1CRHo
"""

#IR BERT RUNTIME PREDICTION xgboost based

import pandas as pd
import numpy as np
import zipfile
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import torch
import csv

# Unzip embeddings.zip
with zipfile.ZipFile('/content/drive/MyDrive/Research /Clemson/Embeddings/syr2k_recreations_embeddings.zip', 'r') as zip_ref:
    zip_ref.extractall('embeddings_folder')

# Specify the directory containing your CSV files
csv_directory = '/content/train_directory'  # Replace with the actual path to your CSV files
embeddings_directory = 'embeddings_folder/embeddings/'  # Replace with the actual path to your embeddings

# Initialize lists to store embeddings and runtimes
embeddings = []
runtimes = []

# Iterate through CSV files in the specified directory
for file_name in os.listdir(csv_directory):
    if file_name.endswith('.csv'):
        file_path = os.path.join(csv_directory, file_name)

        # Load the CSV file
        df = pd.read_csv(file_path)

        # Iterate through the CSV file to extract embeddings and runtimes
        for index, row in df.iterrows():
            filename = row['filename'] + '_embeddings.pth'
            print(filename)
            # new_embedding_name = row['filename'] + '.pth'
            # original_embedding_path = os.path.join(embeddings_directory, original_embedding_name)
            new_embedding_path = os.path.join(embeddings_directory, filename)

            # Rename the embedding file
            #os.rename(original_embedding_path, new_embedding_path)

            # Load the embedding
            embedding = torch.load(new_embedding_path).numpy()

            # Append the embedding and runtime to the lists
            embeddings.append(embedding)
            runtimes.append(row['runtime'])  # Replace with the actual column name for runtimes

print(len(embeddings))

print(runtimes[2])

# Convert lists to NumPy arrays
X = np.array(embeddings)
y = np.array(runtimes)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_test.shape)

nsamples, nx, ny = X_train.shape
d2_train_dataset = X_train.reshape((nsamples,nx*ny))

msamples, mx, my = X_test.shape
d2_test_dataset = X_test.reshape((msamples,mx*my))

# Standardize the input data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(d2_train_dataset)
X_test_scaled = scaler.transform(d2_test_dataset)

# Build an XGBoost model
model = xgb.XGBRegressor(objective ='reg:squarederror')

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model performance
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

test_directory = './test_directory'

test_data = []

final_result = []

!pwd

csv_file = "output.csv"

# Open the file in write mode
with open(csv_file, mode='w', newline='') as file:
  # Create a CSV writer
  writer = csv.writer(file)

  # Write the header if needed
  writer.writerow(['filename', 'predicted_runtime','actual_runtime'])

  for file_name in os.listdir(test_directory):
      if file_name.endswith('.pth'):
          # Load the embedding
          test_embedding_path = os.path.join(test_directory, file_name)
          test_embedding = torch.load(test_embedding_path).numpy()

          # Append the embedding and runtime to the lists
          #test_data.append(test_embedding)

          #NEW CODE
          new_embedding_scaled = scaler.transform(test_embedding.reshape(1,-1))
          predicted_runtime = model.predict(new_embedding_scaled)

          res = f'{file_name}: {predicted_runtime[0]}'
          print(res)
          n_name = file_name.replace("_embeddings.pth","")
          print(n_name)
          final_result.append({'filename': n_name, 'predicted_runtime': predicted_runtime[0], 'actual_runtime': None})

          writer.writerow([file_name, predicted_runtime[0]])

print(final_result)

# Iterate through CSV files in the specified directory
for file_name in os.listdir(csv_directory):
    if file_name.endswith('.csv'):
        file_path = os.path.join(csv_directory, file_name)

        # Load the CSV file
        df = pd.read_csv(file_path)

        # Iterate through the CSV file to extract embeddings and runtimes
        for index, row in df.iterrows():
            filename = row['filename']
            for res in final_result:
              print(res['filename'])
              if res['filename'] == filename:
                res['actual_runtime'] = row['runtime']

!python --version

print(final_result)

df = pd.DataFrame(final_result)

df.to_excel('runtime_predictions.xlsx', index=False)

for embedding in test_data:
# Now, you can use the trained model to predict the runtime for a new embedding
  # new_embedding_path = embeddings_directory+'/mmp_syr2k_L_1_embeddings.pth'  # Replace with the path to the new embedding file
  # new_embedding = torch.load(new_embedding_path).numpy()
  new_embedding_scaled = scaler.transform(embedding.reshape(1,-1))
  predicted_runtime = model.predict(new_embedding_scaled)
  print(f'Predicted Runtime for the New Embedding: {predicted_runtime[0]}')

print(f'Predicted Runtime for the New Embedding: {predicted_runtime[0]}')