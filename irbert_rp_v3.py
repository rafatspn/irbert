# -*- coding: utf-8 -*-
"""IRBERT_RP_v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfoVLxAxe_Q-DEwomQyneENwp_jiR-kG
"""

#NN Based implementation

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
from zipfile import ZipFile
import csv

# Define your neural network architecture
class EmbeddingPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(EmbeddingPredictor, self).__init__()
        # print("PRINTING TYPES")
        # print(type(input_size))
        # print(type(hidden_size))
        # print(type(output_size))
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Custom dataset class
class EmbeddingDataset(Dataset):
    def __init__(self, csv_file, zip_file_path):
        self.data = pd.read_csv(csv_file)
        self.zip_file_path = zip_file_path
        self.extracted_folder = "/content/embeddings"

        # Extract the zip file
        with ZipFile(zip_file_path, 'r') as zip_file:
            zip_file.extractall(self.extracted_folder)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        embedding_filename = self.data.iloc[idx]['filename']
        embedding_path = os.path.join(self.extracted_folder + "/embeddings", embedding_filename + "_embeddings.pth")

        # Load the embedding
        with open(embedding_path, 'rb') as file:
            embedding = torch.load(file)

        #runtime = self.data.iloc[idx]['runtime']
        runtime = torch.tensor(self.data.iloc[idx]['runtime'], dtype=torch.float32)

        return {'embedding': embedding, 'runtime': runtime}

# Training function
def train(model, train_loader, criterion, optimizer, num_epochs=10):
    for epoch in range(num_epochs):
        for batch in train_loader:
            embeddings, runtimes = batch['embedding'], batch['runtime']

            outputs = model(embeddings)
            # print("TYPE OF MODEL")
            # print(type(outputs))
            # print(type(runtimes))
            # print(type(embeddings))
            loss = criterion(outputs, runtimes)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Main script
if __name__ == "__main__":
    # Paths
    csv_files_directory = "/content/train_directory"
    zip_file_path = "/content/drive/MyDrive/Research /Clemson/Embeddings/syr2k_recreations_embeddings.zip"
    test_data_directory = "/content/test_directory"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Hyperparameters
    input_size =  768 # Set based on the size of your embeddings
    hidden_size = 64  # Adjust as needed
    output_size = 1  # For predicting runtime

    # Load multiple CSV files and merge them
    all_csv_files = [file for file in os.listdir(csv_files_directory) if file.endswith('.csv')]
    merged_data = pd.concat([pd.read_csv(os.path.join(csv_files_directory, csv_file)) for csv_file in all_csv_files])

    # Save the merged data to a new CSV file
    merged_csv_path = "/content/train_directory/merged_data.csv"
    merged_data.to_csv(merged_csv_path, index=False)

    # Create dataset and DataLoader for the merged data
    dataset = EmbeddingDataset(merged_csv_path, zip_file_path)
    train_data, _ = train_test_split(dataset, test_size=0.2, random_state=42)
    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
    print("PRINTING DATASET")
    print(train_loader.dataset[0]['embedding'][0][0])
    print(type(train_loader.dataset[0]['embedding'][0][0]))
    # Initialize model, loss function, and optimizer
    model = EmbeddingPredictor(input_size, hidden_size, output_size)
    #model = model.double()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Train the model
    train(model, train_loader, criterion, optimizer)

    # Now you can use the trained model to predict runtime for new embeddings in each test data directory
    all_test_embeddings = [file for file in os.listdir(test_data_directory) if file.endswith('.pth')]

    csv_file = "output.csv"

    # Open the file in write mode
    with open(csv_file, mode='w', newline='') as file:
      # Create a CSV writer
      writer = csv.writer(file)

      # Write the header if needed
      writer.writerow(['filename', 'runtime'])

      # Iterate over each test embedding
      for test_embedding_file in all_test_embeddings:
          test_embedding_path = os.path.join(test_data_directory, test_embedding_file)

          # Example: Load test embeddings and predict runtime
          with open(test_embedding_path, 'rb') as file:
              test_embedding = torch.load(file).float()

          model.eval()
          with torch.no_grad():
              predicted_runtime = model(test_embedding)

          print(f'Predicted Runtime for {test_embedding_file}: {predicted_runtime.item()}')
          writer.writerow([test_embedding_file, predicted_runtime.item()])



